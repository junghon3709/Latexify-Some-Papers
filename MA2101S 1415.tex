\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsfonts}
\usepackage{amsmath}

\title{1415SEM2-MA2101S Answers}
\author{Pan Jing Bin, Dick Jessen William (Reviewer)}
\date{May 2020}

\begin{document}

\maketitle
\textbf{Question 1}\\\\
(a)To prove 'if':\\\\
Let $u_1 + U',u_2 + U' \in U/U'$ such that $u_1+U' = u_2+U'$\\\\
Then $u_1-u_2 \in U'$ so $\alpha(u_1-u_2) \in V'$\\\\
$\beta(u_1+U') = \alpha(u_1) + V' = \alpha(u_1) - \alpha(u_1-u_2) + V' = \alpha(u_2) + V' = \beta(u_2 + U')$\\\\
Thus $\beta$ is well defined.\\\\
To prove 'only if'\\\\
Let $w\in U'$. Then $w+U' = 0_v + U'$. Since $\beta$ is well-defined:\\\\
$\beta(w+U') = \beta(0_v + U') \to \alpha(w) + V' = 0_v + V'$\\\\
Thus $\alpha(w) \in V'$ so $\alpha(U')\subseteq V'$\\\\
(b)(i) Let $u+U',v+U'\in U/U'$ and $x,y\in \mathbb{F}$
\begin{align*}
\beta(xu + yv + U') = \alpha(xu+yv) + V'  &= x\alpha(u) + y\alpha(v) + V' \ (\text{Since } \alpha \text{ is a linear transformation})\\ &= x\beta(u+U') + y\beta(v+U')
\end{align*}
Thus $\beta$ is linear.\\\\
(ii) To prove 'if':\\\\
Let $u+U' \in \ker(\beta)$. Then $\beta(u+U') = 0_v + V' \to \alpha(u) \in V'$.\\\\
Thus $u\in\alpha^{-1}(V')$. Since $\alpha^{-1}(V')\subseteq U', u\in U'$ so $u+U' = 0_v + U'$\\\\
$\ker(\beta) = \{0_v\}$ so $\beta$ is injective.\\\\
To prove 'only if':\\\\
Let $v\in\alpha^{-1}(V')$. Then $\alpha(v) \in V'$\\\\
$\beta(v+U') = 0_v + V'$ so $v+U'\in \ker(\beta)$. By injectivity of $\beta$,
$v + U' = 0_v + U'$\\\\Thus $v\in U'$ so $\alpha^{-1}(V') \subseteq U'$.\\\\
(iii) To prove 'if':\\\\
Let $w+V' \in V/V'$.\\\\Since $\alpha(U) + V' = V,$ we can write $w = u + v$ for $u\in \alpha(U), v \in V'$\\\\
$u\in\alpha(U) \to \exists u' \in U$ such that $ \alpha(u') = u$\\\\Then $u' + U' \in U/U'$ and $\beta(u'+U') = \alpha(u') + V' = u + V' = w + V'$ \\\\ Thus $\beta$ is surjective. \\\\
To prove 'only if':\\\\
$\alpha(U)\subseteq V \land V' \subseteq V \to \alpha(U) + V' \subseteq V$. Thus it suffice to prove $V\subseteq\alpha(U) + V'.$\\\\
Let $k\in V.$ By surjectivity of $\beta, \exists k'+U' \in U/U'$ such that $\beta(k'+U') = k + V'$\\\\
$\alpha(k') + V' = k + V' \to k-\alpha(k') \in V'$\\\\
Thus we can write: $k = k - \alpha(k') + \alpha(k')$ for $k-\alpha(k')\in V'$ and $\alpha(k')\in \alpha(U)$.\\\\
But this means that $k\in \alpha(U) + V'.$ Hence $V\subseteq \alpha(U) + V'$\\\\\\
\textbf{Question 2}\\\\
(a) We will only prove that $U_1 = $ span$(\{u_1,\alpha(u_1),\alpha^2(u_1),...\}).$ The proof for $U_2$ is similiar.\\\\
Obviously span$(\{u_1,\alpha(u_1),\alpha^2(u_1),...\}) \subseteq U_1$ since $U_1$ is $\alpha$-invariant. Thus it suffice to prove $U_1 \subseteq$ span$(\{u_1,\alpha(u_1),\alpha^2(u_1),...\})$. Let $w\in U_1$
\begin{align*}
w&=c_0v + c_1\alpha(v) + c_2\alpha^2(v) + ... + c_n\alpha^n(v) \text{ - Since } V = \text{span}(\{v,\alpha(v),\alpha^2(v),...\})\\
&=c_0(u_1+u_2) + c_1\alpha(u_1+u_2) + c_2\alpha^2(u_1+u_2) + ... + c_n\alpha^n(u_1+u_2)\\
&=[c_0u_1 + c_1\alpha(u_1) + ... + c_n\alpha^n(u_1)] + [c_0u_2 + c_1\alpha(u_2) + ... + c_n\alpha^n(u_2)]
\end{align*}
Since $U_1$ and $U_2$ are $\alpha$-invariant subspaces:\\\\ $[c_0u_1 + c_1\alpha(u_1) + ... + c_n\alpha^n(u_1)]\in U_1 \land [c_0u_2 + c_1\alpha(u_2) + ... + c_n\alpha^n(u_2)]\in U_2$\\\\
We can also write: $w = w + 0_v$ for $w\in U_1, 0_v\in U_2.$ Since $U_1 + U_2$ is a direct sum, we have:\\\\
$c_0u_1 + c_1\alpha(u_1) + ... + c_n\alpha^n(u_1) = w \ , \ 
c_0u_2 + c_1\alpha(u_2) + ... + c_n\alpha^n(u_2) = 0_v$.\\\\
Thus $w\in$ span$(\{u_1,\alpha(u_1),\alpha^2(u_1),...\})$ so $U_1 \subseteq$ span$(\{u_1,\alpha(u_1),\alpha^2(u_1),...\}).$\\\\
(b)(i) Similarly, we only prove the case for $i=1$. Since $V = $ span$(\{v,\alpha(v),\alpha^2(v),...\}),\\ \exists r(x) \in F[x]$ such that $r(\alpha)(v) = u_1$. If deg$(r(x)) < $ deg$(m(x))$, then we are done. If deg$(r(x)) \geq $ deg$(m(x)$), then we perform the Euclidean Algorithm:\\\\
$r(x) - b(x)m(x) = q_1(x)$ for some $b(x),q_1(x)\in F[x]\  \land \  $deg$(q_1(x)) <  $ deg$(m(x))$\begin{align*}
q_1(\alpha)(v) &= r(\alpha)(v) - b(\alpha)m(\alpha)(v) \\&= r(\alpha)(v) - 0_v \text{  (By definition of minimial polynomial)} \\&= u_1 \text{ (As desired)}
\end{align*}\\
(ii) Claim: $q_1(\alpha) + q_2(\alpha) = I_v$\\\\
Proof: Let $\alpha^k(v) \in \{v,\alpha(v),\alpha^2(v),...\}$\begin{align*}
q_1(\alpha)(v) + q_2(\alpha)(v) &= u_1 + u_2 = v\\ q_1(\alpha)(\alpha^k(v)) + q_2(\alpha)(\alpha^k(v)) &= \alpha^k[q_1(\alpha)(v) + q_2(\alpha)(v)] \\ &= \alpha^k(v)
\end{align*}
Since $V = $ span$(\{v,\alpha(v),\alpha^2(v),...\}),$ and $[q_1(\alpha)+q_2(\alpha)](\alpha^k(v)) = I(\alpha^k(v))\\ \forall \alpha^k(v) \in \{v,\alpha(v),\alpha^2(v)...\},$ we conclude that $q_1(\alpha) + q_2(\alpha) = I_v.$\\\\
$q_1(x) + q_2(x) - 1$ is a polynomial of degree less than $m(x).$\\\\
But $q_1(\alpha) + q_2(\alpha) - I_v = 0v.$ Thus $q_1(x) + q_2(x) - 1 = 0$ (Otherwise it contradicts the definition of minimal polynomial) Hence we get: $q_1(x) + q_2(x) = 1$\\\\
$q_1(\alpha)(u_1+u_2) = u_1 + 0_v \to q_1(\alpha)(u_1) + q_1(\alpha)(u_2) = u_1 + 0v.$\\\\ Recall that $q_1(\alpha)(u_1) \in U_1 \land q_2(\alpha)(u_2) \in U_2$ since $U_1$ and $U_2$ are $\alpha$-invariant. By the unique expression property of direct sums, $q_1(\alpha)(u_1) = u_1 \land q_1(\alpha)(u_2) = 0_v$\\\\
$q_1(\alpha)(q_2(\alpha)(v)) = q_1(\alpha)(u_2) = 0_v$\\\\
(iii) From part(b), we know that : $q_1(\alpha)(u_2) = 0_v.$\\\\ Since $U_2 = $ span$(\{u_2,\alpha(u_2),\alpha^2(u_2),...\}), q_1(\alpha)(k) = 0_v\ \forall k\in U_2$\\\\
Thus by definition of minimial polynomial, $p_2(x)|q_1(x).$ Similarly, $p_1(x)|q_2(x).$\\\\ But gcd$(q_1(x),q_2(x))= 1$ since $q_1(x) + q_2(x) = 1$. Thus $p_1(x)$ and $p_2(x)$ must be coprime as well.\\\\
(c) Let $p_1(x),p_2(x)$ denote the minimal polynomial of $\alpha$ restricted on $U_1$ and $U_2$ respectively. Since $f(\alpha)^k(v) = 0$ and $V =$ span$(\{v,\alpha(v)\alpha^2(v),...\}), \\ f(\alpha)^k(t) = 0\ \forall t\in V$. By definition of minimial polynomial, $m_\alpha(x)|f(x)^k.$\\\\
Thus $p_1(x) = f(x)^{k_1}, p_2(x) = f(x)^{k_2}$ for $ 0\leq k_1\leq k, 0\leq k_2\leq k.$\\ If $k_1 > 0 \land k_2 > 0,$ then $gcd(p_1(x),p_2(x)) \neq 1,$ which contradicts (b)(iii). Hence $k_1 = 0 \lor k_2 = 0$ so $U_1 = \{0\}$ or $ U_2 = \{0\}.$\\\\\\
\textbf{Question 3}\\\\
(a) For any arbitrary $A\in SL_2(\mathbb{F}_p), $ the first column of $A$ can be any column except the zero column. (Which will result in $\det(A) = 0$) Thus there are $p^2-1$ choices.\\\\
For the second column of $A$, consider 2 cases:\\\\
Case 1: $a = 0 \lor c = 0\\\\$Without loss of generality, assume $a= 0 \land c \neq 0$.\\\\
Then $d$ can be any element while there is only 1 choice for $b$, which is $-c^{-1}.$ Thus there are $p$ choices for the second column of $A$.\\\\
Case 2: $a\neq 0 \land c \neq 0$\\\\
Then $d$ can be any element and for each $d$ there is only 1 choice for $b$, which is $adc^{-1}-c^{-1}$. Similiar to case 1, there are $p$ choices for the second column of $A.$\\\\
In total, there are $(p^2-1)p = p^3-p$ elements in $SL_2(\mathbb{F}_p)$.\\\\
(b) Let $A\in SL_2(\mathbb{F}_p)$ and consider 2 cases.\\\\
Case 1: $c_A(x) = m_A(x)$
\\\\
Since $\det(A) = 1, c_A(x) = x^2 + ax + 1$ for $a\in \mathbb{F}_p$\\ Then $A$ is similar to $R$ (Rational canonical form):\\\\\begin{center}
$R =$
\begin{pmatrix}
0 & -1\\
1 & -a 
\end{pmatrix}, where $C_A(x) = x^2 +ax+1$\\\\
\end{center}\\\\
There are $p$ choices for $a$ so there are $p$ pairwise non-similar matrices of this form. (Note that changing the value of $a$ will result in a non-similar matrix since the characteristic polynomial of $A$ have changed)\\\\
Case 2: $c_A(x) \neq m_A(x)$\\\\
Then $c_A(x) = (x-\lambda)^2$ for some $\lambda \in \mathbb{F}_p$. Since $\det(A)=1,\lambda^2 = 1$
\\\\
If $\mathbb{F}_p$ has characteristic greater than $2$, then $\lambda^2 = 1$ have $2$ solutions:\\ $\lambda = 1 \lor \lambda = -1$.\\ Thus there are 2 matrices that $A$ can be similar to: \begin{pmatrix}
1 & 0\\
0 & 1
\end{pmatrix} and \begin{pmatrix}
-1 & 0\\
0 & -1
\end{pmatrix}\\\\
If $\mathbb{F}_p$ has characteristic $2$, then $\lambda^2 = 1$ have only $1$ solution:\\ $\lambda = 1.$ (Since $-1 = 1$)\\ Thus there is only 1 matrix that $A$ can be similar to: \begin{pmatrix}
1 & 0\\
0 & 1
\end{pmatrix}\\
In conclusion, there are $p+2$ pairwise non-similar matrices when char$(\mathbb{F}_p) \neq 2$ and $p+1$ pairwise non-similar matrices when char$(\mathbb{F}_p) = 2$.\\\\
\textbf{Question 4}\\\\
(a) Let $(p,q),(r,s)$ denote the index of positively of $\phi$ and $\psi$ respectively.\\\\
Claim: $p \leq r$\\\\
Proof: Let $M_\phi,M_\psi$ denote the maximal subspace of $V$ such that $\phi_{|_{M_\phi\times M_\phi}}$ and $\psi_{|_{M_\psi\times M_\psi}}$ are positive definite. Then $\dim(M_\phi) = p \land \dim(M_\psi) = r$.\\\\
Since $\phi(v,v) \leq \psi(v,v),\ \phi(v,v) > 0 \to \psi(v,v) > 0$.\\\\
Thus $M_\phi \subseteq M_\psi$ so we have $\dim(M_\phi) \leq \dim(M_\psi)$ and $p\leq r$.\\\\
Claim: $q\geq s$\\\\
Proof: Let $N_\phi,N_\psi$ denote the maximal subspace of $V$ such that $\phi_{|_{N_\phi\times N_\phi}}$ and $\psi_{|_{N_\psi\times N_\psi}}$ are negative definite. Then $\dim(N_\phi) = q \land \dim(N_\psi) = s$.\\\\
Since $\phi(v,v) \leq \psi(v,v),\  \psi(v,v) < 0 \to \phi(v,v) < 0$.\\\\Thus $ N_\psi \subseteq N_\phi$ so we have $\dim(N_\phi) \geq \dim(N_\psi)$ and $q\geq s$.\\\\
Combining the two claims, we have: $p-q\leq r-s $ so $s_\phi \leq s_\psi$.\\\\
(b) Existence: Let $B=\{w_1,w_2,...w_n\}$ be a basis for $W$ and let $C$ and $D$ be the representing matrix of $\theta$ and $\chi$ under basis $B$ respectively. (Note that $W$ is finite-dimensional). Since $\chi$ is non-degenerate, $D$ is invertible so $D^{-1}$ exists. Choose $\alpha$ to be the linear operator such that:\begin{center}
    $[\alpha]_B = D^{-1}C$
\end{center}
Then we have:\\\\ $\theta(x,y) = ([x]_B)^TC[y]_B = ([x]_B)^TDD^{-1}C[y]_B = ([x]_B)^TD[\alpha(y)]_B = \chi(x,\alpha(y))$\\\\
Uniqueness: Let $\alpha_1,\alpha_2$ both be linear operators on $W$ such that:\begin{center}
    $\chi(x,\alpha_1(y)) = \chi(x,\alpha_2(y)) = \theta(x,y)\ \forall x,y\in W$ 
\end{center}
Let $A_1,A_2$ be the standard matrix of $\alpha_1$ and $\alpha_2$ under basis B respectively.\\\\
$\forall x,y\in W, \ ([x]_B)^TDA_1[y]_B = ([x]_B)^TDA_2[y]_B$\\\\
This equality holds for all $x,y \in W,$ so $ DA_1 = DA_2.$ Since D is invertible, $A_1 = A_2$.  Since $\alpha_1$ and $\alpha_2$ have the same standard matrix under basis $B$, we conclude that $\alpha_1 = \alpha_2$.


\end{document}
